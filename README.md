# model_training-supervised_fine_tuning

# ðŸ¦™ Alpaca + GPTâ€‘Neoâ€‘125M Fineâ€‘Tuning

This project demonstrates **supervised fineâ€‘tuning (SFT)** of the Hugging Face model **EleutherAI/gptâ€‘neoâ€‘125M** on the **Alpaca dataset**.  
The goal is to adapt a base language model to follow instructions more effectively by training it on `(instruction, input, output)` pairs.

---

## ðŸ“Œ Project Overview
- **Base Model**: `EleutherAI/gpt-neo-125M` (causal language model, lightweight and fast).
- **Dataset**: [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) (52k instructionâ€‘response pairs).
- **Frameworks**: Hugging Face `transformers`, `datasets`, `accelerate`.
- **Training Objective**: Nextâ€‘token prediction on formatted instructionâ€‘response text.
- **Hardware**: Runs on GPU (CUDA/MPS) or TPU (via PyTorch/XLA).

---

## âš™ï¸ Setup

Install dependencies:
```bash
pip install -U transformers datasets accelerate sentencepiece huggingface_hub

ðŸ“‚ Data Preparation
1. Load the Alpaca dataset:

from datasets import load_dataset
dataset = load_dataset("tatsu-lab/alpaca")

2. Format examples into a single text string:

def format_example(example):
    if example.get("input"):
        prompt = f"Instruction: {example['instruction']}\nInput: {example['input']}\nOutput:"
    else:
        prompt = f"Instruction: {example['instruction']}\nOutput:"
    return {"text": prompt + example["output"]}

3. Tokenize with GPTâ€‘Neo tokenizer:

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neo-125M")
tokenizer.pad_token = tokenizer.eos_token  # Fix padding issue

ðŸ‹ï¸ Training
Use Hugging Face Trainer:

from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling

model = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-neo-125M")

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    learning_rate=5e-5,
    logging_dir="./logs",
    logging_steps=100,
    save_steps=500,
    warmup_steps=100,
    weight_decay=0.01,
    bf16=True,   # âœ… use bf16 on TPU, fp16 on CUDA
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset.select(range(1000)),  # small eval subset
    tokenizer=tokenizer,
    data_collator=data_collator
)

trainer.train()

ðŸ“Š Evaluation
After training, test inference:

from transformers import pipeline

pipe = pipeline("text-generation", model="./results", tokenizer=tokenizer)
print(pipe("Instruction: Write a poem about F1 racing\nOutput:", max_new_tokens=100)[0]["generated_text"])

ðŸ–¥ï¸ Hardware Notes
â€¢ Mac (MPS): Use small batch sizes (2â€“4). Models >1.3B params may OOM.
â€¢ GPU (CUDA): Enable fp16=True for faster training.
â€¢ TPU (v5eâ€‘8): Use bf16=True and optim="adamw_torch_xla" if supported. Large batch sizes (16â€“32) are possible.

---
ðŸš€ Key Learnings
â€¢ GPTâ€‘Neo doesnâ€™t define a pad token â†’ must set tokenizer.pad_token = tokenizer.eos_token.
â€¢ On TPU, avoid fused optimizers â†’ use adamw_torch_xla.
â€¢ Hugging Face Trainer simplifies fineâ€‘tuning but requires versionâ€‘compatible arguments.

ðŸ“Œ Next Steps
â€¢ Try larger models (gpt-neo-1.3B, opt-350m) if hardware allows.
â€¢ Experiment with quantization (8â€‘bit/4â€‘bit) for memory efficiency.
â€¢ Evaluate outputs with BLEU/ROUGE or human preference scoring.
